{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15742287",
   "metadata": {},
   "source": [
    "# Debugging mdio\n",
    "\n",
    "In this notebook we will configure an environment that is useful for profiling and debugging mdio segy ingestion and export:\n",
    "\n",
    "- environment\n",
    "- SEGY generation\n",
    "- segy to mdio\n",
    "- mdio to segy\n",
    "\n",
    "Memory issues:\n",
    "https://distributed.dask.org/en/stable/worker-memory.html#memory-not-released-back-to-the-os\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc09589",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "First configure environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd0987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Make sure ipykernel is installed\n",
    "!{sys.executable} -m pip install ipykernel\n",
    "# Install QC tools\n",
    "!{sys.executable} -m pip install matplotlib pandas dask_memusage memray\n",
    "# Make sure mdio is installed\n",
    "!poetry install --extras \"distributed\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928405ed",
   "metadata": {},
   "source": [
    "After the previous cell is run the kernel needs to be restarted so the module gets picked up. Failure to do so will result in the following cell to fail with the error:  `ModuleNotFoundError: No module named 'mdio'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a964af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdio import mdio_to_segy, MDIOReader\n",
    "#import dask.array as dask\n",
    "import dask\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from dask.diagnostics import ProgressBar\n",
    "import time\n",
    "import os\n",
    "from dask.distributed import LocalCluster, Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bc1c7e",
   "metadata": {},
   "source": [
    "### Setup dask cluster\n",
    "\n",
    "\n",
    "For dask applications the flow can use dask_memusage which is a much simpler profiler based on polling.  memray seems to be a recent and significant improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f384d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dask_memusage\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "tmp_path = \"/scratch/tmp2/\"\n",
    "MY_TEMP = tmp_path\n",
    "\n",
    "dask.config.set({\"temporary_directory\": os.path.join(MY_TEMP, \"temp\")})\n",
    "\n",
    "dask.config.set({\"distributed.comm.timeouts.tcp\": \"90s\"})\n",
    "dask.config.set({\"distributed.comm.timeouts.connect\": \"60s\"})\n",
    "\n",
    "num_cut_dask_workers = 2  \n",
    "memory_cut_dask_worker = 60  \n",
    "\n",
    "gb = 1024**3\n",
    "\n",
    "use_dask = True\n",
    "single_process = False\n",
    "\n",
    "if use_dask:\n",
    "    print(\n",
    "        f\"New local cluster. n_workers {num_cut_dask_workers} mem_limit = {memory_cut_dask_worker} Gb\"\n",
    "    )\n",
    "    with dask.config.set({\"distributed.scheduler.worker-saturation\": 1.0}):\n",
    "        if single_process:\n",
    "            client = Client(processes=False) \n",
    "        else:\n",
    "            cluster = LocalCluster(\n",
    "                n_workers=num_cut_dask_workers,\n",
    "                threads_per_worker=1,\n",
    "                memory_limit=memory_cut_dask_worker * gb,\n",
    "            )\n",
    "\n",
    "            client = Client(cluster)\n",
    "else:\n",
    "    client = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6abf00f",
   "metadata": {},
   "source": [
    "### Setup monitoring dashboard for dask\n",
    "\n",
    "The dask dashboard should automatically be setup on http://127.0.0.1:8787/status.  The [configuration for the dev container](../.devcontainer/devcontainer.json) should have the port forwarding setup for this port enabling this to be viewed.  The following cell will also give a summary of the client.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b959f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5194907",
   "metadata": {},
   "source": [
    "#### Check python and mdio versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e89ed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python path: {sys.executable}\")\n",
    "import mdio\n",
    "print(f\"mdio version: {mdio.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a4ee0e",
   "metadata": {},
   "source": [
    "## SEGY generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa333a",
   "metadata": {},
   "source": [
    "#### Functions to generate segy files based on tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6826ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test configuration before everything runs.\"\"\"\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pytest\n",
    "import segyio\n",
    "\n",
    "from mdio.segy.geometry import StreamerShotGeometryType\n",
    "def create_segy_mock_6d(\n",
    "    fake_segy_tmp: str,\n",
    "    num_samples: int,\n",
    "    shots: list,\n",
    "    cables: list,\n",
    "    receivers_per_cable: list,\n",
    "    shot_lines: list = [  # noqa:  B006\n",
    "        1,\n",
    "    ],\n",
    "    comp_types: list = [  # noqa:  B006\n",
    "        1,\n",
    "    ],\n",
    "    chan_header_type: StreamerShotGeometryType = StreamerShotGeometryType.A,\n",
    "    index_receivers: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"Dummy 6D SEG-Y file for use in tests.\n",
    "\n",
    "    Data will be created with:\n",
    "\n",
    "    offset is byte location 37 - offset 4 bytes\n",
    "    fldr is byte location 9 - shot 4 byte\n",
    "    ep is byte location 17 - shot 4 byte\n",
    "    stae is byte location 137 - cable 2 byte\n",
    "    tracf is byte location 13 - channel 4 byte\n",
    "    styp is byte location 133 - shot_line 2 byte\n",
    "    afilf is byte location 141 - comptype 2 byte\n",
    "\n",
    "    \"\"\"\n",
    "    spec = segyio.spec()\n",
    "    segy_file = fake_segy_tmp\n",
    "\n",
    "    shot_count = len(shots)\n",
    "    total_chan = np.sum(receivers_per_cable)\n",
    "    trace_count_per_line = shot_count * total_chan\n",
    "    sline_count = len(shot_lines)\n",
    "    comp_trace_count = trace_count_per_line * sline_count\n",
    "    comp_count = len(comp_types)\n",
    "    trace_count = comp_trace_count * comp_count\n",
    "\n",
    "    spec.format = 1\n",
    "    spec.samples = range(num_samples)\n",
    "    spec.tracecount = trace_count\n",
    "    spec.endian = \"big\"\n",
    "\n",
    "    # Calculate shot, cable, channel/receiver numbers and header values\n",
    "    cable_headers = []\n",
    "    channel_headers = []\n",
    "\n",
    "    # TODO: Add strict=True and remove noqa when minimum Python is 3.10\n",
    "    for cable, num_rec in zip(cables, receivers_per_cable):  # noqa: B905\n",
    "        cable_headers.append(np.repeat(cable, num_rec))\n",
    "\n",
    "        channel_headers.append(np.arange(num_rec) + 1)\n",
    "\n",
    "    cable_headers = np.hstack(cable_headers)\n",
    "    channel_headers = np.hstack(channel_headers)\n",
    "\n",
    "    if chan_header_type == StreamerShotGeometryType.B:\n",
    "        channel_headers = np.arange(total_chan) + 1\n",
    "\n",
    "    index_receivers = True\n",
    "    if chan_header_type == StreamerShotGeometryType.C:\n",
    "        index_receivers = False\n",
    "\n",
    "    shot_headers = np.hstack([np.repeat(shot, total_chan) for shot in shots])\n",
    "    cable_headers = np.tile(cable_headers, shot_count)\n",
    "    channel_headers = np.tile(channel_headers, shot_count)\n",
    "\n",
    "    # Add shot lines\n",
    "    shot_line_headers = np.hstack(\n",
    "        [np.repeat(shot_line, trace_count_per_line) for shot_line in shot_lines]\n",
    "    )\n",
    "\n",
    "    shot_headers = np.tile(shot_headers, sline_count)\n",
    "    cable_headers = np.tile(cable_headers, sline_count)\n",
    "    channel_headers = np.tile(channel_headers, sline_count)\n",
    "\n",
    "    # Add multiple components\n",
    "    comptype_headers = np.hstack(\n",
    "        [np.repeat(comp, comp_trace_count) for comp in comp_types]\n",
    "    )\n",
    "\n",
    "    shot_line_headers = np.tile(shot_line_headers, comp_count)\n",
    "    shot_headers = np.tile(shot_headers, comp_count)\n",
    "    cable_headers = np.tile(cable_headers, comp_count)\n",
    "    channel_headers = np.tile(channel_headers, comp_count)\n",
    "\n",
    "    with segyio.create(segy_file, spec) as f:\n",
    "        for trc_idx in range(trace_count):\n",
    "            shot = shot_headers[trc_idx]\n",
    "            cable = cable_headers[trc_idx]\n",
    "            channel = channel_headers[trc_idx]\n",
    "            shot_line = shot_line_headers[trc_idx]\n",
    "            comptype = comptype_headers[trc_idx]\n",
    "\n",
    "            # offset is byte location 37 - offset 4 bytes\n",
    "            # fldr is byte location 9 - shot 4 byte\n",
    "            # ep is byte location 17 - shot 4 byte\n",
    "            # stae is byte location 137 - cable 2 byte\n",
    "            # tracf is byte location 13 - channel 4 byte\n",
    "            # styp is byte location 133 - shot_line 2 byte\n",
    "            # afilf is byte location 141 - comptype 2 byte\n",
    "\n",
    "            if index_receivers:\n",
    "                f.header[trc_idx].update(\n",
    "                    offset=0,\n",
    "                    fldr=shot,\n",
    "                    ep=shot,\n",
    "                    stae=cable,\n",
    "                    tracf=channel,\n",
    "                    styp=shot_line,\n",
    "                    afilf=comptype,\n",
    "                )\n",
    "            else:\n",
    "                f.header[trc_idx].update(\n",
    "                    offset=0,\n",
    "                    fldr=shot,\n",
    "                    ep=shot,\n",
    "                    stae=cable,\n",
    "                    styp=shot_line,\n",
    "                    afilf=comptype,\n",
    "                )\n",
    "\n",
    "            samples = np.linspace(start=shot, stop=shot + 1, num=num_samples)\n",
    "            f.trace[trc_idx] = samples.astype(\"float32\")\n",
    "\n",
    "        f.bin.update()\n",
    "\n",
    "    return segy_file\n",
    "\n",
    "def segy_mock_6d_shots(segy_path: str) -> dict[str, str]:\n",
    "    \"\"\"Generate mock 6D shot SEG-Y files.\"\"\"\n",
    "    num_samples = 25\n",
    "    shots = [2, 3, 5]\n",
    "    cables = [0, 101, 201, 301]\n",
    "    receivers_per_cable = [1, 5, 7, 5]\n",
    "    shot_lines = [1, 2, 4, 5, 99]\n",
    "    comp_types = [1, 2, 3, 4]\n",
    "\n",
    "    \n",
    "    chan_header_type =     StreamerShotGeometryType.A,\n",
    "    \n",
    "    segy_path = create_segy_mock_6d(\n",
    "        segy_path,\n",
    "        num_samples=num_samples,\n",
    "        shots=shots,\n",
    "        cables=cables,\n",
    "        receivers_per_cable=receivers_per_cable,\n",
    "        chan_header_type=chan_header_type,\n",
    "        shot_lines=shot_lines,\n",
    "        comp_types=comp_types,\n",
    "    )\n",
    "    return segy_path\n",
    "\n",
    "def segy_mock_4d_shots(segy_path: str) -> dict[str, str]:\n",
    "    \"\"\"Generate mock 4D shot SEG-Y files.\"\"\"\n",
    "    num_samples = 25\n",
    "    shots = [2, 3, 5]\n",
    "    cables = [0, 101, 201, 301]\n",
    "    receivers_per_cable = [1, 5, 7, 5]\n",
    "    shot_lines = [1,]\n",
    "    comp_types = [1,]\n",
    "\n",
    "    \n",
    "    chan_header_type =     StreamerShotGeometryType.A,\n",
    "    \n",
    "    segy_path = create_segy_mock_6d(\n",
    "        segy_path,\n",
    "        num_samples=num_samples,\n",
    "        shots=shots,\n",
    "        cables=cables,\n",
    "        receivers_per_cable=receivers_per_cable,\n",
    "        chan_header_type=chan_header_type,\n",
    "        shot_lines=shot_lines,\n",
    "        comp_types=comp_types,\n",
    "    )\n",
    "    return segy_path\n",
    "\n",
    "def segy_mock_4d_shots_large(segy_path: str, num_shots:int=100) -> dict[str, str]:\n",
    "    \"\"\"Generate mock 4D shot SEG-Y files at a reasonable scale.\"\"\"\n",
    "    num_samples = 4000\n",
    "    num_cables = 12\n",
    "    num_receivers_per_cable = 250 \n",
    "    shots = range(num_shots)\n",
    "    cables = range(num_cables)\n",
    "    receivers_per_cable = [num_receivers_per_cable,]*num_cables\n",
    "    shot_lines = [1,]\n",
    "    comp_types = [1,]\n",
    "\n",
    "    \n",
    "    chan_header_type =     StreamerShotGeometryType.A,\n",
    "    \n",
    "    segy_path = create_segy_mock_6d(\n",
    "        segy_path,\n",
    "        num_samples=num_samples,\n",
    "        shots=shots,\n",
    "        cables=cables,\n",
    "        receivers_per_cable=receivers_per_cable,\n",
    "        chan_header_type=chan_header_type,\n",
    "        shot_lines=shot_lines,\n",
    "        comp_types=comp_types,\n",
    "    )\n",
    "    return segy_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b02400",
   "metadata": {},
   "source": [
    "#### segy config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55229136",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = 4\n",
    "large_segy = True\n",
    "num_shots = 1000\n",
    "\n",
    "if dims == 6:\n",
    "    index_header_names = (\"comptype\", \"shot_line\",\"shot_point\", \"cable\", \"channel\")\n",
    "    index_types = (\"int16\", \"int16\", \"int32\", \"int16\", \"int32\")\n",
    "    index_bytes= (141, 133, 17, 137, 13)\n",
    "    chunksize = (1, 2, 4, 2, 128, 1024)\n",
    "    grid_overrides = {\"AutoChannelWrap\": True}\n",
    "    num_shots = 3\n",
    "    segy_path = os.path.join(tmp_path, f\"segy_{dims}d_{num_shots}.sgy\")\n",
    "    print(segy_path)\n",
    "    access_pattern=\"012345\"\n",
    "elif dims == 4:\n",
    "    index_header_names = (\"shot_point\", \"cable\", \"channel\")\n",
    "    index_types = (\"int32\", \"int16\", \"int32\")\n",
    "    index_bytes= ( 17, 137, 13)\n",
    "    chunksize = (4, 2, 128, 1024)\n",
    "    grid_overrides = {\"AutoChannelWrap\": True}\n",
    "    if large_segy:\n",
    "        segy_path = os.path.join(tmp_path, f\"segy_{dims}d_{num_shots}.sgy\")\n",
    "    \n",
    "    else:\n",
    "        num_shots = 3\n",
    "        segy_path = os.path.join(tmp_path, f\"segy_{dims}d_{num_shots}.sgy\")\n",
    "    access_pattern=\"0123\"\n",
    "    \n",
    "print(segy_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baf43f7",
   "metadata": {},
   "source": [
    "#### Create SEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a40484",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if dims == 6:\n",
    "    segy_path = segy_mock_6d_shots(segy_path)\n",
    "elif dims == 4:\n",
    "    if large_segy:\n",
    "        segy_path = segy_mock_4d_shots_large(segy_path, num_shots=num_shots)\n",
    "    else:\n",
    "        segy_path = segy_mock_4d_shots(segy_path)\n",
    "    \n",
    "print(segy_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aa8c7f",
   "metadata": {},
   "source": [
    "## Ingest segy to mdio\n",
    "\n",
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6f5e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdio_path = os.path.join(tmp_path, f\"segy_{dims}d_import_{num_shots}.mdio\")\n",
    "kwargs = {\n",
    "    'segy_path': segy_path,\n",
    "    'mdio_path_or_buffer': mdio_path,\n",
    "    'index_names': index_header_names,\n",
    "    'index_bytes': index_bytes,\n",
    "    'index_types': index_types,\n",
    "    'chunksize': chunksize,  # (1, chunksize_2d, -1),\n",
    "    'overwrite': True\n",
    "}\n",
    "if grid_overrides is not None:\n",
    "    kwargs['grid_overrides'] = grid_overrides\n",
    "kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab5982f",
   "metadata": {},
   "source": [
    "#### Actual segy to mdio conversion based on config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a622fadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mdio.segy_to_mdio(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43e4fdf",
   "metadata": {},
   "source": [
    "#### QC of generated mdio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa55b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def info(\n",
    "    input_mdio_file,\n",
    "    output_format=\"plain\",\n",
    "    access_pattern=\"012\",\n",
    "):\n",
    "    \"\"\"Provide information on MDIO dataset.\n",
    "    By default this returns human readable information about the grid and stats for\n",
    "    the dataset. If output-format is set to json then a json is returned to\n",
    "    facilitate parsing.\n",
    "    \"\"\"\n",
    "    reader = mdio.MDIOReader(\n",
    "        input_mdio_file, access_pattern=access_pattern, return_metadata=True\n",
    "    )\n",
    "    mdio_dict = {}\n",
    "    mdio_dict[\"grid\"] = {}\n",
    "    for axis in reader.grid.dim_names:\n",
    "        dim = reader.grid.select_dim(axis)\n",
    "        min = dim.coords[0]\n",
    "        max = dim.coords[-1]\n",
    "        size = dim.coords.shape[0]\n",
    "        axis_dict = {\"name\": axis, \"min\": min, \"max\": max, \"size\": size}\n",
    "        mdio_dict[\"grid\"][axis] = axis_dict\n",
    "\n",
    "    if output_format == \"plain\":\n",
    "        print(\"{:<10} {:<10} {:<10} {:<10}\".format(\"NAME\", \"MIN\", \"MAX\", \"SIZE\"))\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        for _, axis_dict in mdio_dict[\"grid\"].items():\n",
    "            print(\n",
    "                \"{:<10} {:<10} {:<10} {:<10}\".format(\n",
    "                    axis_dict[\"name\"],\n",
    "                    axis_dict[\"min\"],\n",
    "                    axis_dict[\"max\"],\n",
    "                    axis_dict[\"size\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        print(\"\\n\\n{:<10} {:<10}\".format(\"STAT\", \"VALUE\"))\n",
    "        print(\"=\" * 20)\n",
    "        for name, stat in reader.stats.items():\n",
    "            print(f\"{name:<10} {stat:<10}\")\n",
    "    if output_format == \"json\":\n",
    "        mdio_dict[\"stats\"] = reader.stats\n",
    "        print(mdio_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc7838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "info(\n",
    "    mdio_path,\n",
    "    output_format=\"plain\",\n",
    "    access_pattern=access_pattern,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feef694",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reader = mdio.MDIOReader(\n",
    "    mdio_path, access_pattern=access_pattern, return_metadata=True\n",
    ")\n",
    "comp_dim = reader.grid.select_dim(index_header_names[0])\n",
    "\n",
    "print(f\"comp_dim: {comp_dim} for {reader}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fffdea",
   "metadata": {},
   "source": [
    "## SEGY export (cut)\n",
    "\n",
    "#### First declare functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a44354",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memray\n",
    "\n",
    "import csv\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from distributed.diagnostics import MemorySampler\n",
    "from distributed.diagnostics.memray import memray_workers\n",
    "\n",
    "\n",
    "def processing_time(end, start):\n",
    "    return (end - start) / 60\n",
    "\n",
    "\n",
    "def file_size(file):\n",
    "    import os\n",
    "\n",
    "    filesize = os.path.getsize(file)\n",
    "    return filesize\n",
    "\n",
    "\n",
    "def make_folders(folder_path):\n",
    "    import os\n",
    "\n",
    "    msg = \"Folder already exists\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        msg = \"Folders created\"\n",
    "    return msg\n",
    "\n",
    "def create_segy(mdio_source, temp_local_destination, client, selection_mask=None, access_pattern=\"0123\"):\n",
    "    start = time.perf_counter()\n",
    "    access_pattern = \"0123\"\n",
    "    print(\"Started_conv\")\n",
    "\n",
    "    _ = psutil.cpu_percent(interval=None, percpu=True)\n",
    "    mdio_to_segy(\n",
    "        mdio_source,\n",
    "        temp_local_destination,\n",
    "        selection_mask=selection_mask,\n",
    "        access_pattern=access_pattern,\n",
    "        client=client,\n",
    "    )\n",
    "    mdio_to_segy_time = time.perf_counter()\n",
    "    cpu_mdio_to_segy = psutil.cpu_percent(interval=None, percpu=True)\n",
    "    max_cpu_mdio_to_sgy = max(cpu_mdio_to_segy)\n",
    "    min_cpu_usage = min(cpu_mdio_to_segy)\n",
    "    cpu_usage_avg = np.mean(np.array(cpu_mdio_to_segy))\n",
    "    print(\"cpu_usage_mdio_to_segy_max\", max_cpu_mdio_to_sgy)\n",
    "    mem_usage_mdio_to_sgy = int(\n",
    "        psutil.virtual_memory().total - psutil.virtual_memory().available\n",
    "    )\n",
    "    return (\n",
    "        max_cpu_mdio_to_sgy,\n",
    "        min_cpu_usage,\n",
    "        cpu_usage_avg,\n",
    "        mem_usage_mdio_to_sgy,\n",
    "        processing_time(mdio_to_segy_time, start),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_max_mem_from_csv(filename: str):\n",
    "    \"\"\"Find maximum memory usage from a dask_memusage memory sampler profiler.\"\"\"\n",
    "    print(f\"mem_file = {filename}\")\n",
    "    try:\n",
    "        mem_df = pd.read_csv(filename)\n",
    "        max_dask_task_memory = int(mem_df[\"max_memory_mb\"].max() * (1024**2))\n",
    "    except:\n",
    "        max_mem_array = []\n",
    "        task_name_array = []\n",
    "        with open(filename) as fp:\n",
    "            Lines = fp.readlines()\n",
    "            for line in Lines:\n",
    "                csv = line.split(',')\n",
    "                if len(csv) > 4:\n",
    "                    max_mem = csv[-1]\n",
    "                    task_name = csv[0]\n",
    "                    try:\n",
    "                        my_mm = float(max_mem)\n",
    "                        max_mem_array.append(my_mm)\n",
    "                        task_name_array.append(task_name)\n",
    "                    except:\n",
    "                        continue\n",
    "        max_index = max_mem_array.index(max(max_mem_array))\n",
    "        print(f\"max_index={max_index} max_mem={max_mem_array[max_index]}MB max_task_name = {task_name_array[max_index]}\")\n",
    "\n",
    "        max_dask_task_memory = int(max_mem_array[max_index] * (1024**2))\n",
    "        return max_dask_task_memory\n",
    "    \n",
    "def get_large_mem_fns_from_csv(filename: str, thresh=50.):\n",
    "    \"\"\"Find functions with a large from a dask_memusage memory sampler profiler.\"\"\"\n",
    "    print(f\"mem_file = {filename}\")\n",
    "    task_name_array = []\n",
    "    with open(filename) as fp:\n",
    "        Lines = fp.readlines()\n",
    "        for line in Lines:\n",
    "            csv = line.split(',')\n",
    "            if len(csv) > 2:\n",
    "                max_mem = csv[-1]\n",
    "                task_name = csv[0]\n",
    "                try:\n",
    "                    my_mm = float(max_mem)\n",
    "                    if my_mm > thresh:\n",
    "                        task_name_array.append(task_name)\n",
    "                except:\n",
    "                    continue\n",
    "    return list(set(task_name_array))\n",
    "\n",
    "        \n",
    "    \n",
    "def plot_function_mem_from_csv(filename: str, fn_name: str):\n",
    "    \"\"\"Plot memory usage for a single function\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    filename: str   csv from dask_memusage memory sampler profiler\n",
    "    fn_name: str    name of function to track and plot\"\"\"\n",
    "    print(f\"mem_file = {filename}\")\n",
    "    mem_array_1 = []\n",
    "    mem_array_2 = []\n",
    "    with open(filename) as fp:\n",
    "        Lines = fp.readlines()\n",
    "        for line in Lines:\n",
    "            csv = line.split(',')\n",
    "            if len(csv) > 4:\n",
    "                max_mem = csv[-1]\n",
    "                max_mem_2 = csv[-2]\n",
    "                task_name = csv[0]\n",
    "                try:\n",
    "                    if fn_name in task_name:\n",
    "                        my_mm = float(max_mem)\n",
    "                        mem_array_1.append(my_mm)\n",
    "                        my_mm = float(max_mem_2)\n",
    "                        mem_array_2.append(my_mm)\n",
    "                except:\n",
    "                    continue\n",
    "    if len(mem_array_1) > 1:\n",
    "        plt.figure()\n",
    "        plt.plot(mem_array_1, label=\"Total\")\n",
    "        plt.plot(mem_array_2, label=\"Proc\")\n",
    "        plt.title(f\"{fn_name}\")\n",
    "        plt.xlabel(\"Occurrence\")\n",
    "        plt.ylabel(\"Memory\")\n",
    "        plt.show\n",
    "        plt.savefig(f'{filename}_{fn_name}.png')\n",
    "    elif len(mem_array_1) == 1:\n",
    "        print(f\"{fn_name}  used {mem_array_1[0]}mb memory\")\n",
    "    else:\n",
    "        print(f\"Had issue reading {fn_name} memory usage\")\n",
    "    return mem_array_1\n",
    "\n",
    "def cut(input_mdio: str, run=0, access_pattern=\"0123\", client=None, test_name=\"6372\"):\n",
    "    \"\"\"Cuts segy from mdio  with memory QC\"\"\"\n",
    "    with open(\n",
    "        os.path.join(MY_TEMP, test_name+\"_metrics_export.csv\"), \"a+\", newline=\"\"\n",
    "    ) as write_obj:\n",
    "        csv_writer = csv.writer(write_obj)\n",
    "        csv_writer.writerow(\n",
    "            [\n",
    "                \"chunk_case\",\n",
    "                \"file_size\",\n",
    "                \"reader_shape\",\n",
    "                \"time\",\n",
    "                \"cpu_usage_max\",\n",
    "                \"cpu_usage_min\",\n",
    "                \"cpu_usage_avg\",\n",
    "                \"mem_usage\",\n",
    "                \"run\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "    print(\"Converting Multidimio to Segy via Local Dask\")\n",
    "\n",
    "    TEMP_DESTINATION = os.path.join(MY_TEMP, test_name+\".sgy\")\n",
    "\n",
    "    print(\"TEMP_DESTINATION is:\", TEMP_DESTINATION)\n",
    "    # Set to true to use dask_memusage to track memory usage\n",
    "    track_memusage = False\n",
    "    # Set to true to use memray to track memory usage\n",
    "    use_memray = False\n",
    "    # Flag to create dask cluster\n",
    "    use_dask = False\n",
    "    if client is not None:\n",
    "        use_dask = True\n",
    "\n",
    "        if use_dask:\n",
    "            print(\n",
    "                f\"New local cluster. n_workers {num_cut_dask_workers} mem_limit = {memory_cut_dask_worker} Gb\"\n",
    "            )\n",
    "            with dask.config.set({\"distributed.scheduler.worker-saturation\": 1.0}):\n",
    "                cluster = LocalCluster(\n",
    "                    n_workers=num_cut_dask_workers,\n",
    "                    threads_per_worker=1,\n",
    "                    memory_limit=memory_cut_dask_worker * gb,\n",
    "                )\n",
    "\n",
    "                client = Client(cluster)\n",
    "            if track_memusage:\n",
    "                mem_file = os.path.join(os.getcwd(), f\"{test_name}_ram_usage.csv\")\n",
    "                dask_memusage.install(client.cluster.scheduler, mem_file)\n",
    "        else:\n",
    "            track_memusage = False\n",
    "            client = None\n",
    "\n",
    "        if track_memusage:\n",
    "                    \n",
    "            if use_memray:\n",
    "                with memray_workers(f\"memray_{test_name}\", report_args=('flamegraph', '--temporal')):\n",
    "                    ms = MemorySampler()\n",
    "\n",
    "                    with ms.sample(test_name):\n",
    "                        (\n",
    "                            cpu_usage_max,\n",
    "                            cpu_usage_min,\n",
    "                            cpu_usage_avg,\n",
    "                            mem_usage,\n",
    "                            time_taken,\n",
    "                        ) = create_segy(input_mdio, TEMP_DESTINATION, client)\n",
    "            else:\n",
    "                with ms.sample(test_name):\n",
    "                        (\n",
    "                            cpu_usage_max,\n",
    "                            cpu_usage_min,\n",
    "                            cpu_usage_avg,\n",
    "                            mem_usage,\n",
    "                            time_taken,\n",
    "                        ) = create_segy(input_mdio, TEMP_DESTINATION, client)\n",
    "        else:\n",
    "            (\n",
    "                cpu_usage_max,\n",
    "                cpu_usage_min,\n",
    "                cpu_usage_avg,\n",
    "                mem_usage,\n",
    "                time_taken,\n",
    "            ) = create_segy(input_mdio, TEMP_DESTINATION, client)\n",
    "\n",
    "    if client is not None:\n",
    "        # mem_file = os.path.join(os.getcwd(), test_name + \"_ram_usage.csv\")\n",
    "        mem_file = os.path.join(MY_TEMP, test_name + \"_ram_usage.csv\")\n",
    "        \n",
    "        dask_memusage.install(client.cluster.scheduler, mem_file)\n",
    "        ms = MemorySampler()\n",
    "\n",
    "        with ms.sample(test_name):\n",
    "            (\n",
    "                cpu_usage_max,\n",
    "                cpu_usage_min,\n",
    "                cpu_usage_avg,\n",
    "                mem_usage,\n",
    "                time_taken,\n",
    "            ) = create_segy(input_mdio, TEMP_DESTINATION, client, access_pattern=access_pattern)\n",
    "        fig = plt.figure()\n",
    "        memory_plot = ms.plot(align=True)\n",
    "        fig = memory_plot.get_figure()\n",
    "        fig.savefig(test_name + \"_.jpg\", bbox_inches=\"tight\")\n",
    "\n",
    "    else:\n",
    "        (\n",
    "            cpu_usage_max,\n",
    "            cpu_usage_min,\n",
    "            cpu_usage_avg,\n",
    "            mem_usage,\n",
    "            time_taken,\n",
    "        ) = create_segy(input_mdio, TEMP_DESTINATION, client, access_pattern=access_pattern)\n",
    "\n",
    "    print(\"cut completed\")\n",
    "\n",
    "    file_size = os.path.getsize(TEMP_DESTINATION) / (1024**3)\n",
    "    print(f\"mdio to segy completed in {time_taken}\")\n",
    "\n",
    "    reader = MDIOReader(\n",
    "        mdio_path_or_buffer=input_mdio, backend=\"dask\", access_pattern=access_pattern,\n",
    "    )\n",
    "    \n",
    "    if client is not None:\n",
    "        # mem_df = pd.read_csv(mem_file)\n",
    "        # max_dask_task_memory = int(mem_df[\"max_memory_mb\"].max() * (1024**2))\n",
    "\n",
    "        max_dask_task_memory = get_max_mem_from_csv(mem_file)\n",
    "        # Find all functions that use a significant amount of memory\n",
    "        large_mem_fns = get_large_mem_fns_from_csv(mem_file)\n",
    "        # Make plots of function memory over time\n",
    "        for fn_name in large_mem_fns:\n",
    "            plot_function_mem_from_csv(mem_file, fn_name)\n",
    "        plot_function_mem_from_csv(mem_file, \"write_to_segy_stack-segy_concat\")\n",
    "        metrics = [\n",
    "            chunksize,\n",
    "            file_size,\n",
    "            reader.shape,\n",
    "            time_taken,\n",
    "            cpu_usage_max,\n",
    "            cpu_usage_min,\n",
    "            cpu_usage_avg,\n",
    "            mem_usage,\n",
    "            max_dask_task_memory,\n",
    "            run,\n",
    "        ]\n",
    "\n",
    "        with open(\n",
    "            os.path.join(MY_TEMP, test_name + \"_metrics_export.csv\"), \"a+\", newline=\"\"\n",
    "        ) as write_obj:\n",
    "            csv_writer = csv.writer(write_obj)\n",
    "            csv_writer.writerow(metrics)\n",
    "\n",
    "        print(f\"{metrics=}\")\n",
    "        os.remove(TEMP_DESTINATION)\n",
    "        time.sleep(30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1d5240",
   "metadata": {},
   "source": [
    "### Run cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5882a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_list = [os.path.join(MY_TEMP,'segy_4d_import_100.mdio'),os.path.join(MY_TEMP,'segy_4d_import_1000.mdio') ]\n",
    "test_name = [\"test_100shots\",\"test_1000shots\"]\n",
    "\n",
    "file_list = [os.path.join(MY_TEMP,'segy_4d_import_1000.mdio'), ]\n",
    "test_name = [\"test_1000shots\",]\n",
    "for mdio_file, test_name in zip(file_list, test_name):\n",
    "    cut(mdio_file, client=client, test_name=test_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2c6d09",
   "metadata": {},
   "source": [
    "Viewing html from memray using HTML Preview plugin fails.  Open in an external web-browser such as chrome. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
